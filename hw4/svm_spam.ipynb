{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C value: 1 current accuracy: 0.96 max accuracy: 0\n",
      "C value: 3 current accuracy: 0.97 max accuracy: 0.96\n",
      "C value: 10 current accuracy: 0.9775 max accuracy: 0.97\n",
      "C value: 30 current accuracy: 0.97625 max accuracy: 0.9775\n",
      "C value: 100 current accuracy: 0.97 max accuracy: 0.9775\n",
      "C value: 300 current accuracy: 0.9725 max accuracy: 0.9775\n",
      "0.9775 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing, metrics\n",
    "import utils\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from linear_classifier import LinearSVM_twoclass\n",
    "\n",
    "# load the SPAM email training dataset\n",
    "\n",
    "X,y = utils.load_mat('data/spamTrain.mat')\n",
    "yy = np.ones(y.shape)\n",
    "yy[y==0] = -1\n",
    "\n",
    "# load the SPAM email test dataset\n",
    "\n",
    "test_data = scipy.io.loadmat('data/spamTest.mat')\n",
    "X_test = test_data['Xtest']\n",
    "y_test = test_data['ytest'].flatten()\n",
    "yytest = np.ones(y_test.shape)\n",
    "yytest[y_test==0] = -1\n",
    "##################################################################################\n",
    "#  YOUR CODE HERE for training the best performing SVM for the data above.       #\n",
    "#  what should C be? What should num_iters be? Should X be scaled?               #\n",
    "#  should X be kernelized? What should the learning rate be? What should the     #\n",
    "#  number of iterations be?                                                      #\n",
    "##################################################################################\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "num_train = 3200;\n",
    "X_train = X[:3200];\n",
    "Xval = X[3200:];\n",
    "yy_train = yy[:3200];\n",
    "yyval = yy[3200:];\n",
    "\n",
    "Cvals = [1,3,10,30,100,300]\n",
    "\n",
    "max_acc = 0;\n",
    "best_C = 0;\n",
    "best_sigma = 0;\n",
    "\n",
    "\n",
    "K = linear_kernel(X_train,X_train)\n",
    "# scale the kernelized data matrix\n",
    "scaler = preprocessing.StandardScaler().fit(K)\n",
    "scaleK = scaler.transform(K)\n",
    "# add the intercept term\n",
    "KK = np.vstack([np.ones((scaleK.shape[0],)),scaleK.T]).T\n",
    "    \n",
    "Kval = linear_kernel(Xval,X_train)\n",
    "# scale the kernelized data matrix\n",
    "scale_Kval = scaler.transform(Kval)\n",
    "# add the intercept term\n",
    "KK_val = np.vstack([np.ones((scale_Kval.shape[0],)),scale_Kval.T]).T    \n",
    "\n",
    "for C in Cvals:\n",
    "    svm = LinearSVM_twoclass()\n",
    "    svm.theta = np.zeros((KK.shape[1],))\n",
    "    svm.train(KK,yy_train,learning_rate=1e-4,reg=C,num_iters=20000,verbose=False,batch_size=KK.shape[0])\n",
    "    pred_val = svm.predict(KK_val)\n",
    "    accuracy = np.sum((pred_val == yyval)*1)/len(yyval)\n",
    "    print(\"C value: \" + str(C),\"current accuracy: \" + str(accuracy),\"max accuracy: \" + str(max_acc))\n",
    "    if (accuracy >= max_acc):\n",
    "        max_acc = accuracy;\n",
    "        best_C = C;\n",
    "print(max_acc,best_C)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 100 current accuracy: 0.91875 max accuracy: 0.9775\n",
      "iteration number: 1000 current accuracy: 0.955 max accuracy: 0.9775\n",
      "iteration number: 4000 current accuracy: 0.97 max accuracy: 0.9775\n",
      "iteration number: 10000 current accuracy: 0.9725 max accuracy: 0.9775\n",
      "iteration number: 20000 current accuracy: 0.9775 max accuracy: 0.9775\n",
      "iteration number: 30000 current accuracy: 0.97375 max accuracy: 0.9775\n",
      "0.9775 20000\n"
     ]
    }
   ],
   "source": [
    "iterations = [100,1000,4000,10000,20000,30000]\n",
    "best_iter = 0;\n",
    "for iters in iterations:\n",
    "    svm = LinearSVM_twoclass()\n",
    "    svm.theta = np.zeros((KK.shape[1],))\n",
    "    svm.train(KK,yy_train,learning_rate=1e-4,reg=best_C,num_iters=iters,verbose=False,batch_size=KK.shape[0])\n",
    "    pred_val = svm.predict(KK_val)\n",
    "    accuracy = np.sum((pred_val == yyval)*1)/len(yyval)\n",
    "    print(\"iteration number: \" + str(iters),\"current accuracy: \" + str(accuracy),\"max accuracy: \" + str(max_acc))\n",
    "    if (accuracy >= max_acc):\n",
    "        max_acc = accuracy;\n",
    "        best_iter = iters;\n",
    "if(best_iter == 0):\n",
    "    best_iter = 20000;\n",
    "print(max_acc,best_iter)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current accuracy: 0.96875 max accuracy: 0.9775\n",
      "current accuracy: 0.97 max accuracy: 0.9775\n",
      "current accuracy: 0.9675 max accuracy: 0.9775\n",
      "current accuracy: 0.9775 max accuracy: 0.9775\n",
      "current accuracy: 0.96 max accuracy: 0.9775\n",
      "current accuracy: 0.93625 max accuracy: 0.9775\n",
      "0.9775 0.0001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lrs = [1e-1,1e-2,1e-3,1e-4,1e-5,1e-6]\n",
    "best_lr = 0;\n",
    "for lr in lrs:\n",
    "    svm = LinearSVM_twoclass()\n",
    "    svm.theta = np.zeros((KK.shape[1],))\n",
    "    svm.train(KK,yy_train,learning_rate=lr,reg=best_C,num_iters=best_iter,verbose=False,batch_size=KK.shape[0])\n",
    "    pred_val = svm.predict(KK_val)\n",
    "    accuracy = np.sum((pred_val == yyval)*1)/len(yyval)\n",
    "    print(\"current accuracy: \" + str(accuracy),\"max accuracy: \" + str(max_acc))\n",
    "    if (accuracy >= max_acc):\n",
    "        max_acc = accuracy;\n",
    "        best_lr = lr;\n",
    "if(best_lr == 0):\n",
    "    best_lr = 1e-4;\n",
    "    \n",
    "print(max_acc,best_lr)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current accuracy: 0.96875 max accuracy: 0.9775\n",
      "current accuracy: 0.97 max accuracy: 0.9775\n",
      "current accuracy: 0.94 max accuracy: 0.9775\n",
      "current accuracy: 0.97375 max accuracy: 0.9775\n",
      "current accuracy: 0.9725 max accuracy: 0.9775\n",
      "current accuracy: 0.95375 max accuracy: 0.9775\n",
      "current accuracy: 0.9675 max accuracy: 0.9775\n",
      "current accuracy: 0.9775 max accuracy: 0.9775\n",
      "current accuracy: 0.96 max accuracy: 0.9775\n",
      "current accuracy: 0.9675 max accuracy: 0.9775\n",
      "current accuracy: 0.97375 max accuracy: 0.9775\n",
      "current accuracy: 0.9575 max accuracy: 0.9775\n",
      "current accuracy: 0.97375 max accuracy: 0.9775\n",
      "current accuracy: 0.97375 max accuracy: 0.9775\n",
      "current accuracy: 0.96875 max accuracy: 0.9775\n",
      "current accuracy: 0.9725 max accuracy: 0.9775\n",
      "current accuracy: 0.97625 max accuracy: 0.9775\n",
      "current accuracy: 0.97 max accuracy: 0.9775\n",
      "current accuracy: 0.96375 max accuracy: 0.9775\n",
      "current accuracy: 0.97625 max accuracy: 0.9775\n",
      "current accuracy: 0.97 max accuracy: 0.9775\n",
      "current accuracy: 0.965 max accuracy: 0.9775\n",
      "current accuracy: 0.97375 max accuracy: 0.9775\n",
      "current accuracy: 0.975 max accuracy: 0.9775\n",
      "current accuracy: 0.96875 max accuracy: 0.9775\n",
      "current accuracy: 0.97 max accuracy: 0.9775\n",
      "current accuracy: 0.97625 max accuracy: 0.9775\n",
      "0.9775 10 20000 0.0001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "Cvals = [10,30,100]\n",
    "iterations = [4000,10000,20000]\n",
    "lrs = [1e-3,1e-4,1e-5]\n",
    "best_lr = 0;\n",
    "best_C = 0;\n",
    "best_iter = 0;\n",
    "for C in Cvals:\n",
    "    for iteration in iterations:\n",
    "        for lr in lrs:\n",
    "            svm = LinearSVM_twoclass()\n",
    "            svm.theta = np.zeros((KK.shape[1],))\n",
    "            svm.train(KK,yy_train,learning_rate=lr,reg=C,num_iters=iteration,verbose=False,batch_size=KK.shape[0])\n",
    "            pred_val = svm.predict(KK_val)\n",
    "            accuracy = np.sum((pred_val == yyval)*1)/len(yyval)\n",
    "            print(\"current accuracy: \" + str(accuracy),\"max accuracy: \" + str(max_acc))\n",
    "            if (accuracy > max_acc):\n",
    "                max_acc = accuracy;\n",
    "                best_lr = lr;\n",
    "                best_C = C;\n",
    "                best_iter = iteration;\n",
    "if(best_lr == 0 and best_C == 0 and best_iter == 0):\n",
    "    best_lr = 1e-4;\n",
    "    best_iter = 20000;\n",
    "    best_C = 10;\n",
    "print(max_acc,best_C,best_iter,best_lr)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 20000 0.0001\n",
      "iteration 0 / 20000: loss 10.000000\n",
      "iteration 100 / 20000: loss 2.456204\n",
      "iteration 200 / 20000: loss 2.026511\n",
      "iteration 300 / 20000: loss 1.775690\n",
      "iteration 400 / 20000: loss 1.608407\n",
      "iteration 500 / 20000: loss 1.505424\n",
      "iteration 600 / 20000: loss 1.424841\n",
      "iteration 700 / 20000: loss 1.363712\n",
      "iteration 800 / 20000: loss 1.314737\n",
      "iteration 900 / 20000: loss 1.274128\n",
      "iteration 1000 / 20000: loss 1.236948\n",
      "iteration 1100 / 20000: loss 1.205123\n",
      "iteration 1200 / 20000: loss 1.175196\n",
      "iteration 1300 / 20000: loss 1.148629\n",
      "iteration 1400 / 20000: loss 1.123458\n",
      "iteration 1500 / 20000: loss 1.101002\n",
      "iteration 1600 / 20000: loss 1.078721\n",
      "iteration 1700 / 20000: loss 1.059668\n",
      "iteration 1800 / 20000: loss 1.041429\n",
      "iteration 1900 / 20000: loss 1.025514\n",
      "iteration 2000 / 20000: loss 1.006274\n",
      "iteration 2100 / 20000: loss 0.990371\n",
      "iteration 2200 / 20000: loss 0.977996\n",
      "iteration 2300 / 20000: loss 0.963073\n",
      "iteration 2400 / 20000: loss 0.949976\n",
      "iteration 2500 / 20000: loss 0.937316\n",
      "iteration 2600 / 20000: loss 0.924640\n",
      "iteration 2700 / 20000: loss 0.914022\n",
      "iteration 2800 / 20000: loss 0.903453\n",
      "iteration 2900 / 20000: loss 0.893325\n",
      "iteration 3000 / 20000: loss 0.883752\n",
      "iteration 3100 / 20000: loss 0.873094\n",
      "iteration 3200 / 20000: loss 0.865103\n",
      "iteration 3300 / 20000: loss 0.856838\n",
      "iteration 3400 / 20000: loss 0.847260\n",
      "iteration 3500 / 20000: loss 0.839413\n",
      "iteration 3600 / 20000: loss 0.831253\n",
      "iteration 3700 / 20000: loss 0.824603\n",
      "iteration 3800 / 20000: loss 0.816501\n",
      "iteration 3900 / 20000: loss 0.809779\n",
      "iteration 4000 / 20000: loss 0.802982\n",
      "iteration 4100 / 20000: loss 0.796612\n",
      "iteration 4200 / 20000: loss 0.789984\n",
      "iteration 4300 / 20000: loss 0.784415\n",
      "iteration 4400 / 20000: loss 0.777511\n",
      "iteration 4500 / 20000: loss 0.771570\n",
      "iteration 4600 / 20000: loss 0.766216\n",
      "iteration 4700 / 20000: loss 0.759893\n",
      "iteration 4800 / 20000: loss 0.754537\n",
      "iteration 4900 / 20000: loss 0.748992\n",
      "iteration 5000 / 20000: loss 0.743946\n",
      "iteration 5100 / 20000: loss 0.739704\n",
      "iteration 5200 / 20000: loss 0.735208\n",
      "iteration 5300 / 20000: loss 0.729899\n",
      "iteration 5400 / 20000: loss 0.725442\n",
      "iteration 5500 / 20000: loss 0.721022\n",
      "iteration 5600 / 20000: loss 0.716687\n",
      "iteration 5700 / 20000: loss 0.712654\n",
      "iteration 5800 / 20000: loss 0.708439\n",
      "iteration 5900 / 20000: loss 0.703822\n",
      "iteration 6000 / 20000: loss 0.699941\n",
      "iteration 6100 / 20000: loss 0.695923\n",
      "iteration 6200 / 20000: loss 0.692839\n",
      "iteration 6300 / 20000: loss 0.688126\n",
      "iteration 6400 / 20000: loss 0.684126\n",
      "iteration 6500 / 20000: loss 0.681091\n",
      "iteration 6600 / 20000: loss 0.677331\n",
      "iteration 6700 / 20000: loss 0.673589\n",
      "iteration 6800 / 20000: loss 0.670571\n",
      "iteration 6900 / 20000: loss 0.667548\n",
      "iteration 7000 / 20000: loss 0.663632\n",
      "iteration 7100 / 20000: loss 0.658373\n",
      "iteration 7200 / 20000: loss 0.655324\n",
      "iteration 7300 / 20000: loss 0.651665\n",
      "iteration 7400 / 20000: loss 0.649092\n",
      "iteration 7500 / 20000: loss 0.645501\n",
      "iteration 7600 / 20000: loss 0.642644\n",
      "iteration 7700 / 20000: loss 0.639732\n",
      "iteration 7800 / 20000: loss 0.638847\n",
      "iteration 7900 / 20000: loss 0.635239\n",
      "iteration 8000 / 20000: loss 0.633455\n",
      "iteration 8100 / 20000: loss 0.626583\n",
      "iteration 8200 / 20000: loss 0.623756\n",
      "iteration 8300 / 20000: loss 0.624357\n",
      "iteration 8400 / 20000: loss 0.618671\n",
      "iteration 8500 / 20000: loss 0.616245\n",
      "iteration 8600 / 20000: loss 0.615062\n",
      "iteration 8700 / 20000: loss 0.612909\n",
      "iteration 8800 / 20000: loss 0.609238\n",
      "iteration 8900 / 20000: loss 0.606822\n",
      "iteration 9000 / 20000: loss 0.604416\n",
      "iteration 9100 / 20000: loss 0.601620\n",
      "iteration 9200 / 20000: loss 0.598986\n",
      "iteration 9300 / 20000: loss 0.596482\n",
      "iteration 9400 / 20000: loss 0.592146\n",
      "iteration 9500 / 20000: loss 0.591112\n",
      "iteration 9600 / 20000: loss 0.586614\n",
      "iteration 9700 / 20000: loss 0.585331\n",
      "iteration 9800 / 20000: loss 0.583121\n",
      "iteration 9900 / 20000: loss 0.581214\n",
      "iteration 10000 / 20000: loss 0.578334\n",
      "iteration 10100 / 20000: loss 0.575763\n",
      "iteration 10200 / 20000: loss 0.573636\n",
      "iteration 10300 / 20000: loss 0.572592\n",
      "iteration 10400 / 20000: loss 0.571053\n",
      "iteration 10500 / 20000: loss 0.568110\n",
      "iteration 10600 / 20000: loss 0.565413\n",
      "iteration 10700 / 20000: loss 0.562548\n",
      "iteration 10800 / 20000: loss 0.560101\n",
      "iteration 10900 / 20000: loss 0.557893\n",
      "iteration 11000 / 20000: loss 0.556030\n",
      "iteration 11100 / 20000: loss 0.554117\n",
      "iteration 11200 / 20000: loss 0.552066\n",
      "iteration 11300 / 20000: loss 0.549174\n",
      "iteration 11400 / 20000: loss 0.547482\n",
      "iteration 11500 / 20000: loss 0.544853\n",
      "iteration 11600 / 20000: loss 0.543027\n",
      "iteration 11700 / 20000: loss 0.541125\n",
      "iteration 11800 / 20000: loss 0.538978\n",
      "iteration 11900 / 20000: loss 0.536846\n",
      "iteration 12000 / 20000: loss 0.534819\n",
      "iteration 12100 / 20000: loss 0.533120\n",
      "iteration 12200 / 20000: loss 0.531274\n",
      "iteration 12300 / 20000: loss 0.530109\n",
      "iteration 12400 / 20000: loss 0.528467\n",
      "iteration 12500 / 20000: loss 0.525549\n",
      "iteration 12600 / 20000: loss 0.525705\n",
      "iteration 12700 / 20000: loss 0.521525\n",
      "iteration 12800 / 20000: loss 0.521745\n",
      "iteration 12900 / 20000: loss 0.520570\n",
      "iteration 13000 / 20000: loss 0.518808\n",
      "iteration 13100 / 20000: loss 0.513885\n",
      "iteration 13200 / 20000: loss 0.515703\n",
      "iteration 13300 / 20000: loss 0.513526\n",
      "iteration 13400 / 20000: loss 0.508769\n",
      "iteration 13500 / 20000: loss 0.508762\n",
      "iteration 13600 / 20000: loss 0.506952\n",
      "iteration 13700 / 20000: loss 0.505197\n",
      "iteration 13800 / 20000: loss 0.503469\n",
      "iteration 13900 / 20000: loss 0.499509\n",
      "iteration 14000 / 20000: loss 0.498128\n",
      "iteration 14100 / 20000: loss 0.496870\n",
      "iteration 14200 / 20000: loss 0.493661\n",
      "iteration 14300 / 20000: loss 0.494342\n",
      "iteration 14400 / 20000: loss 0.493983\n",
      "iteration 14500 / 20000: loss 0.492067\n",
      "iteration 14600 / 20000: loss 0.487686\n",
      "iteration 14700 / 20000: loss 0.488912\n",
      "iteration 14800 / 20000: loss 0.484645\n",
      "iteration 14900 / 20000: loss 0.482772\n",
      "iteration 15000 / 20000: loss 0.483549\n",
      "iteration 15100 / 20000: loss 0.478697\n",
      "iteration 15200 / 20000: loss 0.480401\n",
      "iteration 15300 / 20000: loss 0.477561\n",
      "iteration 15400 / 20000: loss 0.474371\n",
      "iteration 15500 / 20000: loss 0.472710\n",
      "iteration 15600 / 20000: loss 0.474698\n",
      "iteration 15700 / 20000: loss 0.470877\n",
      "iteration 15800 / 20000: loss 0.472023\n",
      "iteration 15900 / 20000: loss 0.470767\n",
      "iteration 16000 / 20000: loss 0.467731\n",
      "iteration 16100 / 20000: loss 0.465991\n",
      "iteration 16200 / 20000: loss 0.466953\n",
      "iteration 16300 / 20000: loss 0.461950\n",
      "iteration 16400 / 20000: loss 0.461069\n",
      "iteration 16500 / 20000: loss 0.460749\n",
      "iteration 16600 / 20000: loss 0.460340\n",
      "iteration 16700 / 20000: loss 0.456896\n",
      "iteration 16800 / 20000: loss 0.456121\n",
      "iteration 16900 / 20000: loss 0.454514\n",
      "iteration 17000 / 20000: loss 0.455629\n",
      "iteration 17100 / 20000: loss 0.453655\n",
      "iteration 17200 / 20000: loss 0.449221\n",
      "iteration 17300 / 20000: loss 0.452066\n",
      "iteration 17400 / 20000: loss 0.450347\n",
      "iteration 17500 / 20000: loss 0.447299\n",
      "iteration 17600 / 20000: loss 0.447979\n",
      "iteration 17700 / 20000: loss 0.444090\n",
      "iteration 17800 / 20000: loss 0.442091\n",
      "iteration 17900 / 20000: loss 0.440372\n",
      "iteration 18000 / 20000: loss 0.440312\n",
      "iteration 18100 / 20000: loss 0.438913\n",
      "iteration 18200 / 20000: loss 0.437080\n",
      "iteration 18300 / 20000: loss 0.435433\n",
      "iteration 18400 / 20000: loss 0.434244\n",
      "iteration 18500 / 20000: loss 0.434405\n",
      "iteration 18600 / 20000: loss 0.432902\n",
      "iteration 18700 / 20000: loss 0.431219\n",
      "iteration 18800 / 20000: loss 0.433087\n",
      "iteration 18900 / 20000: loss 0.431587\n",
      "iteration 19000 / 20000: loss 0.429245\n",
      "iteration 19100 / 20000: loss 0.429341\n",
      "iteration 19200 / 20000: loss 0.427905\n",
      "iteration 19300 / 20000: loss 0.427319\n",
      "iteration 19400 / 20000: loss 0.423340\n",
      "iteration 19500 / 20000: loss 0.422911\n",
      "iteration 19600 / 20000: loss 0.423957\n",
      "iteration 19700 / 20000: loss 0.422178\n",
      "iteration 19800 / 20000: loss 0.421239\n",
      "iteration 19900 / 20000: loss 0.420392\n",
      "max accuracy: 0.9865625\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "# YOUR CODE HERE for testing your best model's performance                       #\n",
    "# what is the accuracy of your best model on the test set? On the training set?  #\n",
    "##################################################################################\n",
    "print(best_C,best_iter,best_lr)\n",
    "K = linear_kernel(X_train,X_train)\n",
    "# scale the kernelized data matrix\n",
    "scaler = preprocessing.StandardScaler().fit(K)\n",
    "scaleK = scaler.transform(K)\n",
    "# add the intercept term\n",
    "KK = np.vstack([np.ones((scaleK.shape[0],)),scaleK.T]).T\n",
    "    \n",
    "Kval = linear_kernel(Xval,X_train)\n",
    "# scale the kernelized data matrix\n",
    "scale_Kval = scaler.transform(Kval)\n",
    "# add the intercept term\n",
    "KK_val = np.vstack([np.ones((scale_Kval.shape[0],)),scale_Kval.T]).T   \n",
    "svm = LinearSVM_twoclass()\n",
    "svm.theta = np.zeros((KK.shape[1],))\n",
    "svm.train(KK,yy_train,learning_rate=best_lr,reg=best_C,num_iters=best_iter,verbose=True,batch_size=KK.shape[0])\n",
    "pred_train = svm.predict(KK)\n",
    "accuracy = np.sum((pred_train == yy_train)*1)/len(yy_train)\n",
    "print(\"max accuracy: \" + str(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "max accuracy: 0.982\n",
      "######## top 15 spam words ########\n",
      "remot\n",
      "clearli\n",
      "otherwis\n",
      "york\n",
      "player\n",
      "mondai\n",
      "wife\n",
      "night\n",
      "hot\n",
      "franc\n",
      "young\n",
      "gt\n",
      "believ\n",
      "info\n",
      "water\n"
     ]
    }
   ],
   "source": [
    "test_data = scipy.io.loadmat('data/spamTest.mat')\n",
    "X_test = test_data['Xtest']\n",
    "y_test = test_data['ytest'].flatten()\n",
    "yytest = np.ones(y_test.shape)\n",
    "yytest[y_test==0] = -1\n",
    "print(yytest.shape)\n",
    "Ktest = linear_kernel(X_test,X_train)\n",
    "# scale the kernelized data matrix\n",
    "scale_Ktest = scaler.transform(Ktest)\n",
    "# add the intercept term\n",
    "KK_test = np.vstack([np.ones((scale_Ktest.shape[0],)),scale_Ktest.T]).T   \n",
    "\n",
    "pred_test = svm.predict(KK_test)\n",
    "accuracy = np.sum((pred_test == yytest)*1)/len(yytest)\n",
    "print(\"max accuracy: \" + str(accuracy))\n",
    "##################################################################################\n",
    "# ANALYSIS OF MODEL: Print the top 15 words that are predictive of spam and for  #\n",
    "# ham. Hint: use the coefficient values of the learned model                     #\n",
    "##################################################################################\n",
    "words, inv_words = utils.get_vocab_dict()\n",
    "\n",
    "\n",
    "print (\"######## top 15 spam words ########\")\n",
    "w = np.dot(svm.theta[1:], X_train).argsort()[::-1]\n",
    "for i in w[:15]:\n",
    "    print (words[i])\n",
    "##################################################################################\n",
    "#                    END OF YOUR CODE                                            #\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
